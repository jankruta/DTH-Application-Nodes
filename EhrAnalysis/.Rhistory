# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("https://github.com/AmunStalder/Digital-Transformation-in-Healthcare-Coding-Examples/blob/main/HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("https://github.com/AmunStalder/Digital-Transformation-in-Healthcare-Coding-Examples/main/HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("/HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
setwd("C:\Users\jan_k\medical_software_development\Digital-Transformation-in-Healthcare-Coding-Examples\HealthNewsTwitter")
tweets <- readLines("/HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
setwd("C:\Users\jan_k\medical_software_development\Digital-Transformation-in-Healthcare-Coding-Examples\HealthNewsTwitter")
tweets <- readLines("Health-News-Tweets/bbchealth.txt")
#Build the corpus
corpus <- Corpus(VectorSource(tweets))
removeURL <- function(x) gsub("http://([[:alnum:]|[:punct:]])+", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#corpus <- tm_map(corpus, content_transformer(tolower))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# add extra stop words for example 'available' or 'via'
myStopwords <- c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
#myStopwords <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun", "jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")
# remove stopwords from corpus
corpus <- tm_map(corpus, removeWords, myStopwords)
#Create a matrix related to the terms. Set the minimum Wordlength to 1 until Infinity
TermMatrix <- TermDocumentMatrix(corpus, control = list(minWordLenght=c(1, Inf)))
t <- removeSparseTerms(TermMatrix, sparse = 0.98)
m <- as.matrix(t)
#Plot the frequent terms
frequent <- rowSums(m)
# Set the frequency to terms in our Matrix which  have a certain amount of Repetitions (30 times)
frequent <- subset(frequent, frequent >= 30)
# Create barplot from the most frequent terms with the wished criteria of the axis
barplot(frequent, las=2)
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
setwd("C:\Users\jan_k\medical_software_development\Digital-Transformation-in-Healthcare-Coding-Examples\HealthNewsTwitter")
tweets <- readLines("Health-News-Tweets/bbchealth.txt")
#Build the corpus
corpus <- Corpus(VectorSource(tweets))
removeURL <- function(x) gsub("http://([[:alnum:]|[:punct:]])+", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#corpus <- tm_map(corpus, content_transformer(tolower))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# add extra stop words for example 'available' or 'via'
myStopwords <- c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
#myStopwords <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun", "jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")
# remove stopwords from corpus
corpus <- tm_map(corpus, removeWords, myStopwords)
#Create a matrix related to the terms. Set the minimum Wordlength to 1 until Infinity
TermMatrix <- TermDocumentMatrix(corpus, control = list(minWordLenght=c(1, Inf)))
t <- removeSparseTerms(TermMatrix, sparse = 0.98)
m <- as.matrix(t)
#Plot the frequent terms
frequent <- rowSums(m)
# Set the frequency to terms in our Matrix which  have a certain amount of Repetitions (30 times)
frequent <- subset(frequent, frequent >= 30)
# Create barplot from the most frequent terms with the wished criteria of the axis
barplot(frequent, las=2)
# Assign the matrix and its scale into a dendrogram
distance <- dist(scale(m))
# Print the distances from one frequent term to another --> Calculate the distance from the words within the document. If distance is high --> same Cluster is unlikely, if distance is low, the oposite.
print(distance, digits = 2)
# Create a hierarchical cluster of the terms to estimate the existing clusters
# The Method "ward.D" is a common clustering method in R to reduce variances for clustering --> tries to keep the possible cluster together in a visual way.
hCluster <- hclust(distance, method = "ward.D")
# by reducing the hang attribute to a negative value, some "hanging" terms can be reduced, which supports the decision making for clusters. The are more or less 12 to 15 possible clusters visualized.
plot(hCluster, hang=-1)
rect.hclust(hCluster, k=12)
#Assign the hierarchical matrix into a new string for performing the nonhierarchical clustering
m1 <- t(m)
# Set the number of Clusters, this variable can be modified
k <- 12
kc <- kmeans(m1, k)
print(kc)
library(tm)
library(cluster)
library(readr)
# Read the file you want to analyze, make sure the Text Mining library is installed
# !! Make sure to set your working directory first! --> setwd("C:/Users/xx/your_folderpath")
tweets <- readLines("Health-News-Tweets/bbchealth.txt")
#Build the corpus
corpus <- Corpus(VectorSource(tweets))
removeURL <- function(x) gsub("http://([[:alnum:]|[:punct:]])+", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#corpus <- tm_map(corpus, content_transformer(tolower))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# add extra stop words for example 'available' or 'via'
myStopwords <- c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
#myStopwords <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun", "jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")
# remove stopwords from corpus
corpus <- tm_map(corpus, removeWords, myStopwords)
#Create a matrix related to the terms. Set the minimum Wordlength to 1 until Infinity
TermMatrix <- TermDocumentMatrix(corpus, control = list(minWordLenght=c(1, Inf)))
t <- removeSparseTerms(TermMatrix, sparse = 0.98)
m <- as.matrix(t)
#Plot the frequent terms
frequent <- rowSums(m)
# Set the frequency to terms in our Matrix which  have a certain amount of Repetitions (30 times)
frequent <- subset(frequent, frequent >= 30)
# Create barplot from the most frequent terms with the wished criteria of the axis
barplot(frequent, las=2)
# Assign the matrix and its scale into a dendrogram
distance <- dist(scale(m))
# Print the distances from one frequent term to another --> Calculate the distance from the words within the document. If distance is high --> same Cluster is unlikely, if distance is low, the oposite.
print(distance, digits = 2)
# Create a hierarchical cluster of the terms to estimate the existing clusters
# The Method "ward.D" is a common clustering method in R to reduce variances for clustering --> tries to keep the possible cluster together in a visual way.
hCluster <- hclust(distance, method = "ward.D")
# by reducing the hang attribute to a negative value, some "hanging" terms can be reduced, which supports the decision making for clusters. The are more or less 12 to 15 possible clusters visualized.
plot(hCluster, hang=-1)
rect.hclust(hCluster, k=12)
#Assign the hierarchical matrix into a new string for performing the nonhierarchical clustering
m1 <- t(m)
# Set the number of Clusters, this variable can be modified
k <- 12
kc <- kmeans(m1, k)
print(kc)
setwd('C:\Users\jan_k\medical_software_development\Digital-Transformation-in-Healthcare-Coding-Examples\EhrAnalysis')
setwd('C:/Users/jan_k/medical_software_development/Digital-Transformation-in-Healthcare-Coding-Examples/EhrAnalysis')
# Make sure the needed libraries are installed and load them within the next step
library(cluster)
library(readr)
library(tm)
library(wordcloud)
library(factoextra)
library(ggplot2)
# Read in the dataset into your dataframe
# !! Make sure to set your working directory first! --> setwd("C:/Users/xx/your_folderpath")
my_data <- read.csv("100-Patients/PatientCorePopulatedTable.txt", sep = "\t", fileEncoding="UTF-8-BOM")
# Get first insight to the dataset. For example, let's have a look about the balance of the PatientRace within the data
ggplot(my_data, aes(x=PatientRace)) + geom_bar()
prop.table(table(my_data$PatientRace))
# Get first insight to the dataset. For example, let's have a look about the balance of the PatientGender within the data
ggplot(my_data, aes(x=PatientGender)) + geom_bar()
prop.table(table(my_data$PatientGender))
# Get first insight to the dataset. For example, let's have a look about the balance of the PatientMaritalStatus within the data
ggplot(my_data, aes(x=PatientMaritalStatus)) + geom_bar()
prop.table(table(my_data$PatientMaritalStatus))
# Calculate the patients current age and safe the values within a new column with numerical data type
my_data$PatientDateOfBirth <- as.Date(my_data$PatientDateOfBirth)
my_data$PatientCurrentAge = as.numeric(difftime(Sys.Date(),my_data$PatientDateOfBirth, units = "weeks"))/52.25
str(my_data)
head(my_data)
# As the IDs have many characters, assign a consecutive Patient Number for being able to visualize the clustering
my_data$PatientNumber <- 1:nrow(my_data)
#check the data
head(my_data)
# Move some columns within their position and define the order within the dataframe
my_data <- my_data [ , c("PatientNumber", "PatientID", "PatientGender", "PatientCurrentAge", "PatientDateOfBirth", "PatientRace", "PatientMaritalStatus", "PatientLanguage", "PatientPopulationPercentageBelowPoverty")]
# Create a scatterplot from only available numeric variables by PatientGender, by exncluding few outliers
plot(my_data$PatientPopulationPercentageBelowPoverty, my_data$PatientCurrentAge, xlab = "Population Poverty", ylab = "Patient Age",xlim=c(0,25), ylim=c(0,105), pch=20, col="red")
points(my_data$PatientPopulationPercentageBelowPoverty[my_data$PatientGender=='Female'], my_data$PatientCurrentAge[my_data$PatientGender=='Female'], pch=20, col="blue")
# Create a scatterplot from only available numeric variables by PatientRace, by exncluding few outliers
plot(my_data$PatientPopulationPercentageBelowPoverty, my_data$PatientCurrentAge, xlab = "Population Poverty", ylab = "Patient Age", xlim=c(0,25), ylim=c(0,105), pch=20, col="red")
points(my_data$PatientPopulationPercentageBelowPoverty[my_data$PatientRace=='African American'], my_data$PatientCurrentAge[my_data$PatientRace=='African American'], pch=20, col="blue")
points(my_data$PatientPopulationPercentageBelowPoverty[my_data$PatientRace=='Asian'], my_data$PatientCurrentAge[my_data$PatientRace=='Asian'], pch=20, col="green")
points(my_data$PatientPopulationPercentageBelowPoverty[my_data$PatientRace=='Unknown'], my_data$PatientCurrentAge[my_data$PatientRace=='Unknown'], pch=20, col="black")
# Compute agglomerative clustering with the help of the euclidean distance
d <- dist(my_data, method = "euclidean")
# Hierarchical clustering
hca <- hclust(d, method = "ward.D2")
# Compuute Agglomerative coefficient
hca$ac
## [1] NULL
# Plot the obtained dendrogram
plot(hca, cex = 0.6, hang = -1, main = "Dendrogram Agglomerative Clustering")
rect.hclust(hca, k=8)
# Compute with another method --> agnes (predefined agglomerative clustering method)
hcagnes <- agnes(my_data, method = "complete")
# Agglomerative coefficient
hcagnes$ac
## [1] 0.9915077
# Plot the obtained dendrogram
pltree(hcagnes, cex = 0.6, hang = -1, main = "Dendrogram Agnes Clustering")
rect.hclust(hcagnes, k=12)
# Compute divisive clustering with the help of the predefined method called "diana"
hcd <- diana(my_data)
# Divise coefficient
hcd$dc
## [1] 0.9898009
# Plot the obtained dendrogram
pltree(hcd, cex = 0.6, hang = -1, main = "Dendrogram Divisive Clustering")
rect.hclust(hcd, k=12)
# Plot the before obtained agglomerative dendrogram with the same amounts of defined clusters
clust <- cutree(hca, k = 8)
# from ‘factoextra’ package --> make sure it is installed and loaded
fviz_cluster(list(data = d, cluster = clust))
# Read in new data regarding described diagnosis
newdata <- read.csv("C:/Users/jan_k/Documents/FHNW/dth/100-Patients/AdmissionsDiagnosesCorePopulatedTable.txt", sep = "\t", fileEncoding="UTF-8-BOM")
# Build the corpus
myCorpus <- Corpus(VectorSource(newdata$PrimaryDiagnosisDescription))
# convert to lower case # myCorpus <- tm_map(myCorpus, tolower)
myCorpus <- tm_map(myCorpus, content_transformer(tolower))
# remove punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)
# remove numbers
myCorpus <- tm_map(myCorpus, removeNumbers)
# add extra stop words for example 'available' or 'via'
myStopwords <- c(stopwords("english"), "available", "via", "classified", "diseases", "disease", "left", "elsewhere", "due", "right", "left", "disorder", "disorders", "type", "chemical", "without", "encounter", "factor", "behavior")
# remove stopwords from corpus
myCorpus <- tm_map(myCorpus, removeWords, myStopwords)
#Create a matrix related to the terms. Set the minimum Wordlength to 2 until Infinity
TermMatrix <- TermDocumentMatrix(myCorpus, control = list(minWordLenght=c(2, Inf)))
t <- removeSparseTerms(TermMatrix, sparse = 0.98)
m <- as.matrix(t)
# Plot the frequent terms
frequent <- rowSums(m)
# Set the frequency to terms in our Matrix which  have a certain amount of Repetitions (at least 3 times)
frequent <- subset(frequent, frequent>=3)
# Create barplot from the most frequent terms with the wished criteria of the axis
barplot(frequent, las=2)
# calculate the frequency of words and sort it by frequency
word.freq <- sort(rowSums(m), decreasing = T)
wordcloud(words = names(word.freq), freq = word.freq, min.freq = 2,
random.order = F)
# Assign the matrix and its scale into a dendrogram
distance <- dist(scale(m))
# Print the distances from one frequent term to another --> Calculate the distance from the words within the document. If distance is high --> same Cluster is unlikely, if distance is low, the opposite.
print(distance, digits = 2)
# Create a hierarchical cluster of the terms to estimate the existing clusters
TermCluster <- hclust(distance, method = "ward.D")
# by reducing the hang attribute to a negative value, some "hanging" terms can be reduced, which supports the decision making for clusters. The are more or less 10 possible clusters visualized.
plot(TermCluster, hang=-1, main = "Dendrogram of Diagnosis Terms")
rect.hclust(TermCluster, k=10)
#Assign the hierarchical matrix into a new string for performing the non-hierarchical clustering with kmeans
m1 <- t(m)
# Set the number of Clusters, this variable can be modified
k <- 10
kc <- kmeans(m1, k)
print(kc)
