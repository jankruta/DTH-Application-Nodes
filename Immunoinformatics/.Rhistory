# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("https://github.com/AmunStalder/Digital-Transformation-in-Healthcare-Coding-Examples/blob/main/HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("https://github.com/AmunStalder/Digital-Transformation-in-Healthcare-Coding-Examples/main/HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
tweets <- readLines("/HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
setwd("C:\Users\jan_k\medical_software_development\Digital-Transformation-in-Healthcare-Coding-Examples\HealthNewsTwitter")
tweets <- readLines("/HealthNewsTwitter/Health-News-Tweets/bbchealth.txt")
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
setwd("C:\Users\jan_k\medical_software_development\Digital-Transformation-in-Healthcare-Coding-Examples\HealthNewsTwitter")
tweets <- readLines("Health-News-Tweets/bbchealth.txt")
#Build the corpus
corpus <- Corpus(VectorSource(tweets))
removeURL <- function(x) gsub("http://([[:alnum:]|[:punct:]])+", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#corpus <- tm_map(corpus, content_transformer(tolower))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# add extra stop words for example 'available' or 'via'
myStopwords <- c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
#myStopwords <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun", "jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")
# remove stopwords from corpus
corpus <- tm_map(corpus, removeWords, myStopwords)
#Create a matrix related to the terms. Set the minimum Wordlength to 1 until Infinity
TermMatrix <- TermDocumentMatrix(corpus, control = list(minWordLenght=c(1, Inf)))
t <- removeSparseTerms(TermMatrix, sparse = 0.98)
m <- as.matrix(t)
#Plot the frequent terms
frequent <- rowSums(m)
# Set the frequency to terms in our Matrix which  have a certain amount of Repetitions (30 times)
frequent <- subset(frequent, frequent >= 30)
# Create barplot from the most frequent terms with the wished criteria of the axis
barplot(frequent, las=2)
# Read the file you want to analyze, make sure the Text Mining library is installed
library(tm)
library(cluster)
library(readr)
#setwd("C:/Users/jan_k/Documents/FHNW/dth/Health-Tweets")
#tweets <- readLines("bbchealth.txt")
setwd("C:\Users\jan_k\medical_software_development\Digital-Transformation-in-Healthcare-Coding-Examples\HealthNewsTwitter")
tweets <- readLines("Health-News-Tweets/bbchealth.txt")
#Build the corpus
corpus <- Corpus(VectorSource(tweets))
removeURL <- function(x) gsub("http://([[:alnum:]|[:punct:]])+", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#corpus <- tm_map(corpus, content_transformer(tolower))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# add extra stop words for example 'available' or 'via'
myStopwords <- c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
#myStopwords <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun", "jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")
# remove stopwords from corpus
corpus <- tm_map(corpus, removeWords, myStopwords)
#Create a matrix related to the terms. Set the minimum Wordlength to 1 until Infinity
TermMatrix <- TermDocumentMatrix(corpus, control = list(minWordLenght=c(1, Inf)))
t <- removeSparseTerms(TermMatrix, sparse = 0.98)
m <- as.matrix(t)
#Plot the frequent terms
frequent <- rowSums(m)
# Set the frequency to terms in our Matrix which  have a certain amount of Repetitions (30 times)
frequent <- subset(frequent, frequent >= 30)
# Create barplot from the most frequent terms with the wished criteria of the axis
barplot(frequent, las=2)
# Assign the matrix and its scale into a dendrogram
distance <- dist(scale(m))
# Print the distances from one frequent term to another --> Calculate the distance from the words within the document. If distance is high --> same Cluster is unlikely, if distance is low, the oposite.
print(distance, digits = 2)
# Create a hierarchical cluster of the terms to estimate the existing clusters
# The Method "ward.D" is a common clustering method in R to reduce variances for clustering --> tries to keep the possible cluster together in a visual way.
hCluster <- hclust(distance, method = "ward.D")
# by reducing the hang attribute to a negative value, some "hanging" terms can be reduced, which supports the decision making for clusters. The are more or less 12 to 15 possible clusters visualized.
plot(hCluster, hang=-1)
rect.hclust(hCluster, k=12)
#Assign the hierarchical matrix into a new string for performing the nonhierarchical clustering
m1 <- t(m)
# Set the number of Clusters, this variable can be modified
k <- 12
kc <- kmeans(m1, k)
print(kc)
setwd("C:\Users\jan_k\medical_software_development\Digital-Transformation-in-Healthcare-Coding-Examples\HealthNewsTwitter")
setwd("C:/Users/jan_k/medical_software_development/Digital-Transformation-in-Healthcare-Coding-Examples/HealthNewsTwitter")
library(tm)
library(cluster)
library(readr)
# Read the file you want to analyze, make sure the Text Mining library is installed
# !! Make sure to set your working directory first! --> setwd("C:/Users/xx/your_folderpath")
tweets <- readLines("Health-News-Tweets/bbchealth.txt")
#Build the corpus
corpus <- Corpus(VectorSource(tweets))
removeURL <- function(x) gsub("http://([[:alnum:]|[:punct:]])+", "", x)
corpus <- tm_map(corpus, content_transformer(removeURL))
#corpus <- tm_map(corpus, content_transformer(tolower))
# remove punctuation
corpus <- tm_map(corpus, removePunctuation)
# remove numbers
corpus <- tm_map(corpus, removeNumbers)
# add extra stop words for example 'available' or 'via'
myStopwords <- c("Mon", "Tue", "Wed", "Thu", "Fri", "Sat", "Sun", "Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec")
#myStopwords <- c("mon", "tue", "wed", "thu", "fri", "sat", "sun", "jan", "feb", "mar", "apr", "may", "jun", "jul", "aug", "sep", "oct", "nov", "dec")
# remove stopwords from corpus
corpus <- tm_map(corpus, removeWords, myStopwords)
#Create a matrix related to the terms. Set the minimum Wordlength to 1 until Infinity
TermMatrix <- TermDocumentMatrix(corpus, control = list(minWordLenght=c(1, Inf)))
t <- removeSparseTerms(TermMatrix, sparse = 0.98)
m <- as.matrix(t)
#Plot the frequent terms
frequent <- rowSums(m)
# Set the frequency to terms in our Matrix which  have a certain amount of Repetitions (30 times)
frequent <- subset(frequent, frequent >= 30)
# Create barplot from the most frequent terms with the wished criteria of the axis
barplot(frequent, las=2)
# Assign the matrix and its scale into a dendrogram
distance <- dist(scale(m))
# Print the distances from one frequent term to another --> Calculate the distance from the words within the document. If distance is high --> same Cluster is unlikely, if distance is low, the oposite.
print(distance, digits = 2)
# Create a hierarchical cluster of the terms to estimate the existing clusters
# The Method "ward.D" is a common clustering method in R to reduce variances for clustering --> tries to keep the possible cluster together in a visual way.
hCluster <- hclust(distance, method = "ward.D")
# by reducing the hang attribute to a negative value, some "hanging" terms can be reduced, which supports the decision making for clusters. The are more or less 12 to 15 possible clusters visualized.
plot(hCluster, hang=-1)
rect.hclust(hCluster, k=12)
#Assign the hierarchical matrix into a new string for performing the nonhierarchical clustering
m1 <- t(m)
# Set the number of Clusters, this variable can be modified
k <- 12
kc <- kmeans(m1, k)
print(kc)
# Create a hierarchical cluster of the terms to estimate the existing clusters
# The Method "ward.D" is a common clustering method in R to reduce variances for clustering --> tries to keep the possible cluster together in a visual way.
hCluster <- hclust(distance, method = "ward.D")
# by reducing the hang attribute to a negative value, some "hanging" terms can be reduced, which supports the decision making for clusters. The are more or less 12 to 15 possible clusters visualized.
plot(hCluster, hang=-1)
rect.hclust(hCluster, k=12)
#Assign the hierarchical matrix into a new string for performing the nonhierarchical clustering
m1 <- t(m)
# Set the number of Clusters, this variable can be modified
k <- 12
kc <- kmeans(m1, k)
print(kc)
setwd("C:/Users/jan_k/medical_software_development/Digital-Transformation-in-Healthcare-Coding-Examples/Immunoinformatics")
library(keras)
library(tidyverse)
library(ggseqlogo)
library(PepTools)
library(Biostrings)
# Read in the file you want to analyze
# !! Make sure to set your working directory first! --> setwd("C:/Users/xx/your_folderpath")
tcr_dat <- read.csv("data/tcell_full_v3.csv", sep = ",")
# filter the loadaed data for the columns you need. We limit the chain of acids to 10
tcr_dat %>% group_by(`Object Type`) %>% summarise(n = n())
tcr_dat <- tcr_dat[nchar(as.character(tcr_dat$Description))==10,]
tcr_dat <- tcr_dat[!grepl("Discontinuous", tcr_dat$`Object Type`),]
tcr_dat <- tcr_dat[!grepl("Non-", tcr_dat$`Object Type`),]
tcr_dat <- tcr_dat[!grepl("B", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("J", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("O", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("U", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("Z", tcr_dat$Description),]
tcr_dat <- tcr_dat[, 2]
ggseqlogo(tcr_dat)
# filter the loadaed data for the columns you need. We limit the chain of acids to 10
tcr_dat %>% group_by(`Object Type`) %>% summarise(n = n())
tcr_dat <- tcr_dat[nchar(as.character(tcr_dat$Description))==10,]
tcr_dat <- tcr_dat[!grepl("Discontinuous", tcr_dat$`Object Type`),]
tcr_dat <- tcr_dat[!grepl("Non-", tcr_dat$`Object Type`),]
tcr_dat <- tcr_dat[!grepl("B", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("J", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("O", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("U", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("Z", tcr_dat$Description),]
library(keras)
library(tidyverse)
library(ggseqlogo)
library(PepTools)
library(Biostrings)
# Read in the file you want to analyze
# !! Make sure to set your working directory first! --> setwd("C:/Users/xx/your_folderpath")
tcr_dat <- read.csv("data/tcell_full_v3.csv", sep = ",")
colnames(tcr_dat) = tcr_dat[1, ] # the first row will be the header
tcr_dat = tcr_dat[-1, ]          # removing the first row.
reqd <- as.vector(c("Object Type","Description")) # Storing the columns I want to extract as a vector
tcr_dat <- tcr_dat[,reqd]       # Extracting only four columns
# filter the loadaed data for the columns you need. We limit the chain of acids to 10
tcr_dat %>% group_by(`Object Type`) %>% summarise(n = n())
tcr_dat <- tcr_dat[nchar(as.character(tcr_dat$Description))==10,]
tcr_dat <- tcr_dat[!grepl("Discontinuous", tcr_dat$`Object Type`),]
tcr_dat <- tcr_dat[!grepl("Non-", tcr_dat$`Object Type`),]
tcr_dat <- tcr_dat[!grepl("B", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("J", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("O", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("U", tcr_dat$Description),]
tcr_dat <- tcr_dat[!grepl("Z", tcr_dat$Description),]
tcr_dat <- tcr_dat[, 2]
ggseqlogo(tcr_dat)
# derive the count matrix
tcr_dat %>% pssm_counts %>% .[1:7,1:10]
# derive the frequency matrix
tcr_dat %>% pssm_freqs %>% .[1:7,1:10]
# derive the bits of information matrix
tcr_dat %>% pssm_freqs %>% pssm_bits %>% .[1:7,1:10]
# filter the loadaed data for the columns you need. We limit the chain of acids to 10
bcr_dat <- read.csv("data/bcell_full_v3.csv", sep = ",")
colnames(bcr_dat) = bcr_dat[1, ] # the first row will be the header
bcr_dat = bcr_dat[-1, ]          # removing the first row.
reqd <- as.vector(c("Object Type","Description")) # Storing the columns I want to extract as a vector
bcr_dat <- bcr_dat[,reqd]       # Extracting only four columns
bcr_dat %>% group_by(`Object Type`) %>% summarise(n = n())
bcr_dat <- bcr_dat[nchar(as.character(bcr_dat$Description))==10,]
bcr_dat <- bcr_dat[!grepl("Discontinuous", bcr_dat$`Object Type`),]
bcr_dat <- bcr_dat[!grepl("Non-", bcr_dat$`Object Type`),]
bcr_dat <- bcr_dat[!grepl("B", bcr_dat$Description),]
bcr_dat <- bcr_dat[!grepl("J", bcr_dat$Description),]
bcr_dat <- bcr_dat[!grepl("O", bcr_dat$Description),]
bcr_dat <- bcr_dat[!grepl("U", bcr_dat$Description),]
bcr_dat <- bcr_dat[!grepl("Z", bcr_dat$Description),]
bcr_dat <- bcr_dat[, 2]
ggseqlogo(bcr_dat)
